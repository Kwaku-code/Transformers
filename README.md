This project implements the Transformer Architecture. It defines the Multi-Head Attention layer which uses a scaled dot product between queries and keys to find correlations and similarities between input elements. Also, the architecture which is based on the Multi-Head Attention layer and applies multiple of them in a ResNet-like block. The property of being permutation-equivariant (if we do not provide any positional encodings), which allows it to generalize to many settings.
Then finally, it shows how it can be applied to a simple sequence-to-sequence tasks
